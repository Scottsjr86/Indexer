{"path":"Cargo.toml","lang":"toml","sha1":"0e0ad0ece6343dd6f15e551da1063e5048261e87","size":394,"last_modified":"1757480454","snippet":"[package]\nname = \"indexer\"\nversion = \"0.3.0\"\nedition = \"2021\"\nresolver = \"1\"\n[dependencies]\nanyhow = \"1.0.98\"\nbase64 = \"0.22.1\"\nchrono = { version = \"0.4.41\", features = [\"serde\"] }\nhex = \"0.4.3\"\nignore = \"0.4.23\"\nmemchr = \"2.7.5\"\nquote = \"1.0.40\"\nserde = { version = \"1.0.219\", features = [\"derive\"] }\nserde_json = \"1.0.142\"\nsha1 = \"0.10.6\"\nsha2 = \"0.10.9\"\nsyn = \"2.0.106\"\nwalkdir = \"2.5.0\"","tags":["toml","dir:cargo.toml","ext:toml"],"summary":"Cargo manifest / workspace configuration.","token_estimate":90,"role":"config","module":"Cargo","imports":[],"exports":[],"lines_total":21,"lines_nonblank":19,"rel_dir":"Cargo.toml","noise":false}
{"path":"src/chunker.rs","lang":"rust","sha1":"85a081ed6c4e8b24efaeebb10eeb90c18ba7fac4","size":12604,"last_modified":"1757173442","snippet":"Chunk builder: converts a JSONL index (FileIntentEntry per line) into\nGPT-ready paste chunks, enforcing token caps and splitting large files.\n//! Chunk builder: converts a JSONL index (FileIntentEntry per line) into\n//! GPT-ready paste chunks, enforcing token caps and splitting large files.\nuse anyhow::{Context, Result};\nuse chrono::Utc;\nuse serde::Deserialize;\nuse std::{\n    cmp,\n#[derive(Debug, Deserialize, Clone)]\nstruct FileIntentEntry {\n    pub path: String,\n    #[serde(default)]\n    pub token_estimate: usize,\n/// Build markdown \"paste chunks\" for LLMs from a JSONL index.\n/// - `index_path`: path to JSONL with one FileIntentEntry per line\n/// - `out_prefix`: prefix for output files, e.g. \".gpt/chunks/paste_\"\n/// - `token_cap`: desired approximate token cap per chunk (min 256)\npub fn chunk_index_for_gpt(index_path: &Path, out_prefix: &str, token_cap: usize) -> Result<()> {\n    let token_cap = token_cap.max(256);\nfn load_entries(index_path: &Path) -> Result<Vec<FileIntentEntry>> {\n    let file = File::open(index_path)?;\n#[derive(Debug, Clone)]\nstruct Part {\n    path: String,\nfn split_entry_into_parts(\n    e: &FileIntentEntry,\nfn write_chunk(out_prefix: &str, idx: usize, parts: &[Part]) -> Result<()> {\n    let path = format!(\"{}{}.md\", out_prefix, idx);\nfn render_file_section(out: &mut File, parts: &[Part]) -> Result<()> {\n    if parts.is_empty() {\nfn estimate_tokens_fallback(s: &str) -> usize {\n    let chars = s.len();\nfn fence_lang<'a>(lang: &'a str) -> &'a str {\n    let l = lang.trim();\nfn count_unique_files(parts: &[Part]) -> usize {\n    let mut n = 0usize;\n#[cfg(test)]\nmod tests {\n    use super::*;\n    #[test]\n    fn token_estimator_floor() {\n        assert!(estimate_tokens_fallback(\"\") >= 12);\n    #[test]\n    fn split_small_is_single_part() {\n        let e = FileIntentEntry {\n    #[test]\n    fn split_large_makes_multiple_parts() {\n        let mut body = String::new();","tags":["rust","dir:src","ext:rs","chunk"],"summary":"Splits indexed files into GPT-ready paste chunks.","token_estimate":314,"role":"test","module":"chunker","imports":["anyhow::{Context, Result}","chrono::Utc","serde::Deserialize","std::{","super::*"],"exports":["chunk_index_for_gpt"],"lines_total":409,"lines_nonblank":363,"rel_dir":"src","noise":false}
{"path":"src/custom_view.rs","lang":"rust","sha1":"ef44db6b1ce731abda51ed77b24876e3e3eb049a","size":15653,"last_modified":"1757255502","snippet":"custom_view.rs — \"custom index blocks\" extracted from your source files.\nBlock grammar:\n//--functions public\n$ # Project Functions\n$ *Functions and methods by module. Signatures are shown verbatim (one line).*\n//--end\nCategories: types|functions (aliases: type, structs, enums, fn, fns).\n`$` lines are emitted verbatim. Generated content for the file follows.\nAdd to Cargo.toml deps used by this module:\nanyhow, serde, serde_json, syn, quote\n//! custom_view.rs — \"custom index blocks\" extracted from your source files.\n//!\n//! Block grammar:\n//!   //--functions public\n//!   $ # Project Functions\n//!   $ *Functions and methods by module. Signatures are shown verbatim (one line).*\n//!   //--end\n//!\n//! Categories: types|functions (aliases: type, structs, enums, fn, fns).\n//! `$` lines are emitted verbatim. Generated content for the file follows.\n//!\n//! Add to Cargo.toml deps used by this module:\n//! anyhow, serde, serde_json, syn, quote\nuse std::fs;\nuse std::io;\nuse std::path::{Path, PathBuf};\nuse serde::Deserialize;\nuse syn::visit::Visit;\n#[derive(Debug, Deserialize)]\nstruct FileIntentEntryMini {\n    path: String,\n    #[allow(dead_code)]\n    lang: Option<String>,\n#[derive(Debug, Clone)]\nstruct Section {\n    category: String,\npub fn build_custom_from_index(index_path: &Path, output_path: &Path) -> io::Result<()> {\n    let text = fs::read_to_string(index_path)?;\nfn scan_custom_regions(text: &str, lang: &str) -> Vec<Section> {\n    let mut out = Vec::new();\nfn normalize_category(c: &str) -> String {\n    match c.to_ascii_lowercase().as_str() {\nfn category_heading(c: &str) -> String {\n    match c {\nfn types_for_file(path: &Path) -> Result<String, Box<dyn std::error::Error>> {\n    let src = fs::read_to_string(path)?;\nfn functions_for_file(path: &Path) -> Result<String, Box<dyn std::error::Error>> {\n    let src = fs::read_to_string(path)?;\nfn resolve_path(root: &Path, p: &str) -> PathBuf { let pb = PathBuf::from(p); if pb.is_absolute() { pb } else { root.join(pb) } }\nfn to_rel(root: &Path, p: &Path) -> PathBuf { pathdiff::diff_paths(p, root).unwrap_or_else(|| p.to_path_buf()) }\nmod pathdiff {\n    use std::path::{Component, Path, PathBuf};\n    pub fn diff_paths(path: &Path, base: &Path) -> Option<PathBuf> {","tags":["rust","dir:src","ext:rs"],"summary":"Filesystem / IO utilities.","token_estimate":380,"role":"lib","module":"custom_view","imports":["std::fs","std::io","std::path::{Path, PathBuf}","serde::Deserialize","syn::visit::Visit","std::path::{Component, Path, PathBuf}"],"exports":["build_custom_from_index","diff_paths"],"lines_total":432,"lines_nonblank":390,"rel_dir":"src","noise":false}
{"path":"src/diff.rs","lang":"rust","sha1":"3485f6de849368d852d5d512f3d6915156c3759e","size":8311,"last_modified":"1757173096","snippet":"Compute a structured diff between two index snapshots.\n- Adds / Removes / Modifies (by sha1 change or signal deltas)\n- Renames (one-to-one sha1 match where old path disappeared and new path appeared)\n- Stable, path-sorted output for deterministic diffs\nuse serde_json::{json, Value};\nuse std::collections::{BTreeMap, HashMap, BTreeSet};\nuse crate::file_intent_entry::{FileIntentEntry};\n/// Compute a structured diff between two index snapshots.\n/// - Adds / Removes / Modifies (by sha1 change or signal deltas)\n/// - Renames (one-to-one sha1 match where old path disappeared and new path appeared)\n/// - Stable, path-sorted output for deterministic diffs\npub fn diff_indexes(old: &[FileIntentEntry], new: &[FileIntentEntry]) -> Value {\n    // Index by path\n/// Minimal JSON for a file to keep diff payloads lean.\nfn json_min(e: &FileIntentEntry) -> Value {\n    json!({\n/// Focused delta payload for a modified file (or signal-only change).\nfn json_delta(path: &str, before: &FileIntentEntry, after: &FileIntentEntry) -> Value {\n    json!({\n/// Return true if non-content “signals” changed (lang/role/module/lines/tags).\nfn signals_changed(a: &FileIntentEntry, b: &FileIntentEntry) -> bool {\n    a.lang != b.lang\nfn tags_added(before: &[String], after: &[String]) -> Value {\n    let b: BTreeSet<&str> = before.iter().map(|s| s.as_str()).collect();\nfn tags_removed(before: &[String], after: &[String]) -> Value {\n    let b: BTreeSet<&str> = before.iter().map(|s| s.as_str()).collect();","tags":["rust","dir:src","ext:rs"],"summary":"Compute a structured diff between two index snapshots.","token_estimate":258,"role":"lib","module":"diff","imports":["serde_json::{json, Value}","std::collections::{BTreeMap, HashMap, BTreeSet}","crate::file_intent_entry::{FileIntentEntry}"],"exports":["diff_indexes"],"lines_total":213,"lines_nonblank":195,"rel_dir":"src","noise":false}
{"path":"src/file_intent_entry.rs","lang":"rust","sha1":"e80bcbd9063064861a146765bac1309363887b28","size":13845,"last_modified":"1757205826","snippet":"File-level intent record: what is this file, what does it export, and how should GPT treat it?\nBackward-compat:\n- `#[serde(default)]` keeps old JSONL readable (missing new fields).\n- `role` accepts legacy string values (case-insensitive); unknown -> Role::Other.\nZero extra deps beyond `serde`.\n//! File-level intent record: what is this file, what does it export, and how should GPT treat it?\n//!\n//! Backward-compat:\n//! - `#[serde(default)]` keeps old JSONL readable (missing new fields).\n//! - `role` accepts legacy string values (case-insensitive); unknown -> Role::Other.\n//!\n//! Zero extra deps beyond `serde`.\nuse serde::{\n    de::{\nuse std::fmt;\n/// Coarse role for retrieval/ranking. Keep small & stable.\n#[derive(Copy, Clone, Debug, PartialEq, Eq)]\npub enum Role {\n    Bin, Lib, Test, Doc, Config, Script, Ui, Core, Other,\nimpl Role {\n    pub fn from_str_ic<S: AsRef<str>>(s: S) -> Self {\n        match s.as_ref().to_ascii_lowercase().as_str() {\n    pub fn as_str(self) -> &'static str {\n        match self {\nimpl From<String> for Role {\n    fn from(s: String) -> Self { Role::from_str_ic(&s) }\n}\nimpl From<&str> for Role {\n    fn from(s: &str) -> Self { Role::from_str_ic(s) }\n}\nimpl Default for Role {\n    fn default() -> Self { Role::Other }\n}\nimpl fmt::Display for Role {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        use Role::*;\n        let s = match self {\n/// Primary record emitted per file. This is your JSONL unit.\n#[derive(serde::Serialize, serde::Deserialize, Clone, Debug)]\n#[serde(default)]\npub struct FileIntentEntry {\n    #[serde(deserialize_with = \"de_string_from_any\")] pub path: String,\n    #[serde(deserialize_with = \"de_string_from_any\")] pub lang: String,\n    #[serde(deserialize_with = \"de_string_from_any\")] pub sha1: String,\n    pub size: usize,","tags":["rust","dir:src","ext:rs"],"summary":"File-level intent record: what is this file, what does it export, and how should GPT treat it?","token_estimate":334,"role":"lib","module":"file_intent_entry","imports":["serde::{","std::fmt","Role::*"],"exports":["Role","from_str_ic","as_str","FileIntentEntry"],"lines_total":410,"lines_nonblank":374,"rel_dir":"src","noise":false}
{"path":"src/functions_view.rs","lang":"rust","sha1":"3361cf888dbe4d4c2130864775689c0a67a48968","size":9904,"last_modified":"1757460830","snippet":"functions_view.rs — renders \"Project Functions\" grouped by file and\nsplit into Public / Internal / Tests sections. Method names are prefixed\nwith `Type::` when inside impl blocks. Signatures are one-line, verbatim.\nAdd to Cargo.toml:\n```toml\n[dependencies]\nanyhow = \"1\"\nserde = { version = \"1\", features = [\"derive\"] }\nserde_json = \"1\"\nwalkdir = \"2\"\nsyn = { version = \"2\", features = [\"full\", \"extra-traits\", \"printing\"] }\nquote = \"1\"\n```\n//! functions_view.rs — renders \"Project Functions\" grouped by file and\n//! split into Public / Internal / Tests sections. Method names are prefixed\n//! with `Type::` when inside impl blocks. Signatures are one-line, verbatim.\n//!\n//! Add to Cargo.toml:\n//! ```toml\n//! [dependencies]\n//! anyhow = \"1\"\n//! serde = { version = \"1\", features = [\"derive\"] }\n//! serde_json = \"1\"\n//! walkdir = \"2\"\n//! syn = { version = \"2\", features = [\"full\", \"extra-traits\", \"printing\"] }\n//! quote = \"1\"\n//! ```\nuse std::collections::BTreeMap;\nuse std::fs;\nuse std::io::{self, Write};\nuse std::path::{Path, PathBuf};\nuse serde::Deserialize;\nuse syn::{visit::Visit, ImplItem, Item, ItemFn, ItemImpl};\n#[derive(Debug, Deserialize)]\nstruct FileIntentEntryMini {\n    path: String,\n    #[allow(dead_code)]\n    lang: Option<String>,\npub fn build_functions_from_index(index_path: &Path, output_path: &Path) -> io::Result<()> {\n    // Load entries from JSONL (one object per line), but also accept a JSON array fallback.\n#[derive(Default)]\npub struct FnCollector {\n    pub out: Vec<(Kind, String)>,\n#[derive(Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub enum Kind {\n    Public,\n#[derive(Default)]\npub struct Groups {\n    pub public: Vec<String>,\nimpl Groups {\n    pub fn extend(&mut self, it: impl Iterator<Item = (Kind, String)>) {\n        for (k, s) in it {","tags":["rust","dir:src","ext:rs"],"summary":"Filesystem / IO utilities.","token_estimate":343,"role":"lib","module":"functions_view","imports":["std::collections::BTreeMap","std::fs","std::io::{self, Write}","std::path::{Path, PathBuf}","serde::Deserialize","syn::{visit::Visit, ImplItem, Item, ItemFn, ItemImpl}"],"exports":["build_functions_from_index","FnCollector","Kind","Groups","extend"],"lines_total":327,"lines_nonblank":296,"rel_dir":"src","noise":false}
{"path":"src/helpers.rs","lang":"rust","sha1":"16962d868fa03556c2d65811baee0dcaee8657ae","size":13382,"last_modified":"1757173100","snippet":"Heuristics and light parsers used across scan + intent.\n- Role inference (typed) from path/lang/snippet\n- Module ID derivation (language-aware)\n- Cheap import/export skimming (no regex/AST)\n- Small utilities (dedup, ident capture)\n//! Heuristics and light parsers used across scan + intent.\n//! - Role inference (typed) from path/lang/snippet\n//! - Module ID derivation (language-aware)\n//! - Cheap import/export skimming (no regex/AST)\n//! - Small utilities (dedup, ident capture)\nuse crate::file_intent_entry::Role;\n/// Infer a coarse role for the file: bin/lib/test/doc/config/script/ui/core\n/// Returns a typed `Role`. Pure, allocation-free (except small to_lowercase() temps).\npub fn infer_role(path: &str, lang: &str, snippet: &str) -> Role {\n    let p = path.replace('\\\\', \"/\").to_ascii_lowercase();\n/// Best-effort module id (path → module), language-aware. Returns stable identifiers.\npub fn infer_module_id(path: &str, lang: &str) -> String {\n    let p = path.replace('\\\\', \"/\").trim_matches('/').to_string();\n/// Rust:\n/// - src/lib.rs        -> crate\n/// - src/main.rs       -> bin\n/// - src/bin/foo.rs    -> bin::foo\n/// - src/foo/bar.rs    -> foo::bar\n/// - src/foo/mod.rs    -> foo\npub fn rust_module_id(p: &str) -> String {\n    if p.ends_with(\"src/lib.rs\") { return \"crate\".into(); }\n/// Python: strip extension, convert / to .\n/// tests/foo_test.py -> tests.foo_test\npub fn python_module_id(p: &str) -> String {\n    let stem = p.strip_suffix(\".py\").unwrap_or(p);\n/// Web (ts/js): strip extension; treat directories as namespaces with `::`\npub fn web_module_id(p: &str) -> String {\n    let stem = p.rsplit_once('.').map(|(a, _)| a).unwrap_or(p);\n/// Generic: strip extension; use `::` as separator.\npub fn generic_module_id(p: &str) -> String {\n    let stem = p.rsplit_once('.').map(|(a, _)| a).unwrap_or(p);\n/// Extract imports/exports cheaply from the snippet (no regex/AST).\n/// Returns (imports, exports). Deduplicated, order-preserving (first occurrence).\npub fn skim_symbols(snippet: &str, lang: &str) -> (Vec<String>, Vec<String>) {\n    match lang.to_ascii_lowercase().as_str() {\npub fn skim_rust(s: &str) -> (Vec<String>, Vec<String>) {\n    let mut imports = Vec::new();\npub fn skim_python(s: &str) -> (Vec<String>, Vec<String>) {\n    let mut imports = Vec::new();\npub fn skim_js_ts(s: &str) -> (Vec<String>, Vec<String>) {\n    let mut imports = Vec::new();","tags":["rust","dir:src","ext:rs"],"summary":"Formatting and shared helper utilities.","token_estimate":412,"role":"lib","module":"helpers","imports":["crate::file_intent_entry::Role"],"exports":["infer_role","infer_module_id","rust_module_id","python_module_id","web_module_id","generic_module_id","skim_symbols","skim_rust","skim_python","skim_js_ts"],"lines_total":342,"lines_nonblank":306,"rel_dir":"src","noise":false}
{"path":"src/index_v3.rs","lang":"rust","sha1":"9673f89752892aa8ea6e15974ad80c6b44e76d04","size":9075,"last_modified":"1757475190","snippet":"use std::{fs, path::{Path}};\nuse anyhow::{Context, Result};\nuse base64::{engine::general_purpose::STANDARD as B64, Engine as _};\nuse serde::Serialize;\nuse sha2::{Digest, Sha256};\nuse crate::scan::read_index;\nuse syn::{Item, ItemStruct, ItemEnum, ItemImpl, ImplItem, ItemFn};\n#[derive(Serialize)]\npub struct IndexPack {\n  format: &'static str,\n#[derive(Serialize)]\nstruct LangMeta { primary: &'static str, dialect: &'static str }\n#[derive(Serialize)]\nstruct Rules {\n  mode: &'static str,\n#[derive(Serialize)]\nstruct PatchContract {\n  diff_format: &'static str,\n#[derive(Serialize)]\nstruct FileEntry {\n  path: String,\n#[derive(Serialize)]\nstruct ChunkSet {\n  chunk_size_bytes: usize,\n#[derive(Serialize)]\nstruct Chunk { index: usize, offset: usize, length: usize, sha256: String }\n#[derive(Serialize)]\nstruct Anchor {\n  kind: &'static str,\n#[derive(Serialize)]\nstruct Range { start_line: usize, end_line: usize }\n#[derive(Serialize)]\nstruct Schema {\n  fields: Option<Vec<Field>>,\n#[derive(Serialize)]\nstruct Field { name: String, ty: String, public: bool }\npub fn build_index_v3(index_path: &Path, project_root: &Path, out_path: &Path) -> Result<()> {\n  let entries = read_index(index_path).context(\"read_index\")?; // JSONL or JSON array\nfn hex256(data: impl AsRef<[u8]>) -> String {\n  let mut h = Sha256::new(); h.update(data.as_ref()); hex::encode(h.finalize())\nfn chunk_and_merkle(bytes: &[u8]) -> (Vec<Chunk>, String) {\n  let mut list = Vec::new();","tags":["rust","dir:src","ext:rs"],"summary":"use std::{fs, path::{Path}};","token_estimate":219,"role":"lib","module":"index_v3","imports":["std::{fs, path::{Path}}","anyhow::{Context, Result}","base64::{engine::general_purpose::STANDARD as B64, Engine as _}","serde::Serialize","sha2::{Digest, Sha256}","crate::scan::read_index","syn::{Item, ItemStruct, ItemEnum, ItemImpl, ImplItem, ItemFn}"],"exports":["IndexPack","build_index_v3"],"lines_total":276,"lines_nonblank":263,"rel_dir":"src","noise":false}
{"path":"src/intent.rs","lang":"rust","sha1":"5aa79496bd3b4478e0821d49d0e18eee5b5edd14","size":11014,"last_modified":"1757173101","snippet":"Max characters of `snippet` we scan for intent signals and doc extraction.\nKeep small for speed; we bias toward the top-of-file semantics.\n/// Max characters of `snippet` we scan for intent signals and doc extraction.\n/// Keep small for speed; we bias toward the top-of-file semantics.\nconst MAX_SCAN_BYTES: usize = 32 * 1024;\n/// Public entrypoint: return a short, high-signal, human/GPT friendly summary\n/// for a given file path, snippet, and language label.\npub fn guess_summary(path: &str, snippet: &str, lang: &str) -> String {\n    // Normalize path (Windows-safe) and lowercased mirrors.\n#[inline]\nfn s(msg: &str) -> String { msg.to_string() }\n#[inline]\nfn contains(hay: &str, needle: &str) -> bool { hay.contains(needle) }\n#[inline]\nfn ends_with(hay: &str, suffix: &str) -> bool { hay.ends_with(suffix) }\n#[inline]\nfn _starts_with(hay: &str, prefix: &str) -> bool { hay.starts_with(prefix) }\n#[inline]\nfn ends_with_any(hay: &str, suffixes: &[&str]) -> bool {\n    suffixes.iter().any(|s| hay.ends_with(s))\n#[inline]\nfn any_in(hay: &str, needles: &[&str]) -> bool {\n    needles.iter().any(|n| hay.contains(n))\n#[inline]\nfn eq_ic(a: &str, b: &str) -> bool { a.eq_ignore_ascii_case(b) }\n#[inline]\nfn normalize_path(p: &str) -> String { p.replace('\\\\', \"/\") }\n#[inline]\nfn trim_window(s: &str, max: usize) -> &str {\n    if s.len() > max { &s[..max] } else { s }\nfn is_cargo_toml(pl: &str) -> bool {\n    ends_with(pl, \"cargo.toml\")\nfn is_docker_related(pl: &str) -> bool {\n    ends_with(pl, \"dockerfile\") || contains(pl, \"/docker/\")\nfn is_readme(pl: &str) -> bool {\n    ends_with(pl, \"readme.md\") || ends_with(pl, \"readme\")\nfn is_license(pl: &str) -> bool {\n    ends_with(pl, \"license\") || ends_with(pl, \"license.md\")\nfn is_ci_yaml(pl: &str) -> bool {\n    contains(pl, \".github/workflows/\") || contains(pl, \"/.gitlab-ci\") || contains(pl, \"/.circleci/\")\nfn is_rust_bin_entry(pl: &str, sl: &str) -> bool {\n    ends_with(pl, \"src/main.rs\") || contains(pl, \"/bin/\") || sl.contains(\"fn main(\")\nfn is_python_entry(lang: &str, sl: &str) -> bool {\n    eq_ic(lang, \"python\") && sl.contains(\"if __name__ == '__main__'\")\nfn is_test_file(pl: &str, sl: &str) -> bool {\n    contains(pl, \"/tests\") ||\nfn is_httpish(sl: &str, pl: &str) -> bool {\n    sl.contains(\"axum::\") || sl.contains(\"actix\") || sl.contains(\"rocket::\") || sl.contains(\"warp::\")\nfn is_dblike(sl: &str, pl: &str) -> bool {\n    sl.contains(\"sqlx::\") || sl.contains(\"diesel::\") || sl.contains(\"postgres\")","tags":["rust","dir:src","ext:rs"],"summary":"Intent classifier: offline file purpose inference.","token_estimate":428,"role":"bin","module":"intent","imports":[],"exports":["guess_summary"],"lines_total":286,"lines_nonblank":236,"rel_dir":"src","noise":false}
{"path":"src/lib.rs","lang":"rust","sha1":"ab8a315c5d9f92b46769377d85e37ead8fc3c28a","size":331,"last_modified":"1757460748","snippet":"pub mod util;\npub mod helpers;\npub mod file_intent_entry;\npub mod snippet;\npub mod intent;\npub mod scan;\npub mod chunker;\npub mod types_view;\npub mod diff;\npub mod map_view;\npub mod commands;\npub mod functions_view;\npub mod custom_view;\npub mod index_v3;","tags":["rust","dir:src","ext:rs","crate:lib"],"summary":"Root library file for this Rust crate.","token_estimate":56,"role":"lib","module":"crate","imports":[],"exports":["util","helpers","file_intent_entry","snippet","intent","scan","chunker","types_view","diff","map_view","commands","functions_view","custom_view","index_v3"],"lines_total":18,"lines_nonblank":17,"rel_dir":"src","noise":false}
{"path":"src/main.rs","lang":"rust","sha1":"770ea35b67ba7b8b50e8e65df557a93503aa9fbe","size":97,"last_modified":"1757173103","snippet":"use anyhow::Result;\nfn main() -> Result<()> {\n    indexer::commands::run_cli()","tags":["rust","dir:src","ext:rs","cli","crate:bin"],"summary":"Entrypoint for this Rust binary.","token_estimate":11,"role":"bin","module":"bin","imports":["anyhow::Result"],"exports":[],"lines_total":6,"lines_nonblank":5,"rel_dir":"src","noise":false}
{"path":"src/map_view.rs","lang":"rust","sha1":"8cb4c3eabd4e78f68a1cdabf20b5eb7479427620","size":9164,"last_modified":"1757254025","snippet":"Combined Project Map (with tree-lite appendix)\n- Top section: tag-rich grouped catalog by top-level dir (old MAP).\n- Appendix: compact hierarchical tree (old TREE), same output file.\nOutput path example: `.gpt_index/maps/<slug>_PROJECT_MAP.md`\n//! Combined Project Map (with tree-lite appendix)\n//!\n//! - Top section: tag-rich grouped catalog by top-level dir (old MAP).\n//! - Appendix: compact hierarchical tree (old TREE), same output file.\n//!\n//! Output path example: `.gpt_index/maps/<slug>_PROJECT_MAP.md`\nuse std::{\n    collections::{BTreeMap, BTreeSet},\nuse crate::file_intent_entry::FileIntentEntry;\nuse crate::util;\n/// Public entrypoint: build the combined MAP (+ tree-lite) into `output_path`.\npub fn build_map_from_index(index_path: &Path, output_path: &Path) -> std::io::Result<()> {\n    // Ensure parent dir exists\n#[derive(Clone)]\nstruct EntryLite {\n    path: String,   // relative to top-level group\nfn load_entries(index_path: &Path) -> std::io::Result<Vec<FileIntentEntry>> {\n    let f = File::open(index_path)?;\n/// Return (top_level_dir, remainder_relative_path).\n/// For paths with no '/', group = \"Cargo.toml\" dir-like marker (\".\") and rel=\"filename\".\nfn split_top(path: &str) -> (String, String) {\n    let pb = PathBuf::from(path);\nfn clamp_summary(s: &str) -> String {\n    truncate_ellipsis(s.trim(), 140)\nfn truncate_ellipsis(s: &str, max: usize) -> String {\n    if s.len() <= max {\nfn normalize_tags(tags: &[String]) -> Vec<String> {\n    // keep order, dedup\nfn top_k_tags(freq: &BTreeMap<String, usize>, k: usize) -> (String, usize) {\n    let mut v: Vec<(&str, usize)> = freq.iter().map(|(k, v)| (k.as_str(), *v)).collect();\n#[derive(Default)]\nstruct DirNode {\n    // name is implied by map key; this holds children and file entries\n#[derive(Clone)]\nstruct TreeFile {\n    name: String,  // filename only\n/// Build a directory tree rooted at \"\" from entries.\nfn build_tree(entries: &[FileIntentEntry]) -> DirNode {\n    let mut root = DirNode::default();\n    fn sort_node(n: &mut DirNode) {\n        n.files.sort_by(|a, b| a.name.cmp(&b.name));\n/// Render the tree to markdown (indented bullet list).\nfn render_tree(out: &mut File, node: &DirNode, base: &str, depth: usize) -> std::io::Result<()> {\n    // render current dir header only if depth==0 (root) or base not empty\npub fn indent(depth: usize) -> String {\n    let mut s = String::new();","tags":["rust","dir:src","ext:rs","map"],"summary":"Builds semantic project map (markdown).","token_estimate":406,"role":"lib","module":"map_view","imports":["std::{","crate::file_intent_entry::FileIntentEntry","crate::util"],"exports":["build_map_from_index","indent"],"lines_total":321,"lines_nonblank":282,"rel_dir":"src","noise":false}
{"path":"src/scan.rs","lang":"rust","sha1":"9907c30fe8a61cd76da366d9de85369389f075b4","size":16951,"last_modified":"1757173105","snippet":"Repo scanner: walks the tree, applies ignores, detects language, splits polyglot\ncontainers (HTML), extracts snippets/metadata, and writes a JSONL index.\nDistinctions vs. old version:\n- Uses util::ext_to_lang + shebang for better lang map (Rust, Python, Java, HTML, CSS, JS/TS, etc.).\n- Skips binaries via util::is_probably_binary.\n- Splits HTML into virtual sub-entries for <script> (js/ts) and <style> (css).\n- Deterministic sorting; safer writing via util::safe_write.\n- Tunable limits via ScanOptions.\n- Extra tags (role/module/imports/exports) preserved.\n//! Repo scanner: walks the tree, applies ignores, detects language, splits polyglot\n//! containers (HTML), extracts snippets/metadata, and writes a JSONL index.\n//!\n//! Distinctions vs. old version:\n//! - Uses util::ext_to_lang + shebang for better lang map (Rust, Python, Java, HTML, CSS, JS/TS, etc.).\n//! - Skips binaries via util::is_probably_binary.\n//! - Splits HTML into virtual sub-entries for <script> (js/ts) and <style> (css).\n//! - Deterministic sorting; safer writing via util::safe_write.\n//! - Tunable limits via ScanOptions.\n//! - Extra tags (role/module/imports/exports) preserved.\nuse anyhow::{Context, Result};\nuse ignore::{gitignore::GitignoreBuilder, WalkBuilder};\nuse sha1::{Digest, Sha1};\nuse std::{\n    fs,\nuse crate::{\n    file_intent_entry::FileIntentEntry,\n/// Scan configuration knobs.\n#[derive(Clone, Debug)]\npub struct ScanOptions {\n    /// Hard cap per file (bytes). Files larger than this are skipped.\n    pub max_file_bytes: u64,\n    /// Head bytes for binary sniff.\n    pub sniff_bytes: usize,\n    /// Snippet source window (bytes).\n    pub snippet_bytes: usize,\n    /// Follow symlinks?\n    pub follow_symlinks: bool,\n    /// Include common config/doc types (json/toml/yaml/md)?\n    pub include_docs_and_configs: bool,\n    /// If true, split HTML into sub-entries for <script>/<style>.\n    pub split_html_embeds: bool,\nimpl Default for ScanOptions {\n    fn default() -> Self {\n        Self {\n/// Scan repo and write JSONL index file at `out`.\npub fn scan_and_write_index(root: &Path, out: &Path) -> Result<Vec<FileIntentEntry>> {\n    let mut entries = index_project_with_opts(root, &ScanOptions::default())?;\n/// Default indexer with sane options.\npub fn index_project(root: &Path) -> Result<Vec<FileIntentEntry>> {\n    index_project_with_opts(root, &ScanOptions::default())\n/// Full-control indexer.\npub fn index_project_with_opts(root: &Path, opts: &ScanOptions) -> Result<Vec<FileIntentEntry>> {","tags":["rust","dir:src","ext:rs","scan"],"summary":"Repo scanner: walk, hash, detect, snippet, summarize.","token_estimate":408,"role":"lib","module":"scan","imports":["anyhow::{Context, Result}","ignore::{gitignore::GitignoreBuilder, WalkBuilder}","sha1::{Digest, Sha1}","std::{","crate::{"],"exports":["ScanOptions","scan_and_write_index","index_project","index_project_with_opts"],"lines_total":506,"lines_nonblank":447,"rel_dir":"src","noise":false}
{"path":"src/snippet.rs","lang":"rust","sha1":"32ed897b8389a97ab4239ac4bb7470f7b8bb3c79","size":14093,"last_modified":"1757173106","snippet":"Extract a compact, high-signal snippet optimized for GPT ingestion.\nStrategy:\n1) Capture top-of-file docs/comments (language aware).\n2) Score lines by language; keep highest-signal lines in original order,\nwith a sliver of context after each.\n3) Hard caps and dedup to stay within MAX_KEEP_LINES.\n/// Extract a compact, high-signal snippet optimized for GPT ingestion.\n/// Strategy:\n/// 1) Capture top-of-file docs/comments (language aware).\n/// 2) Score lines by language; keep highest-signal lines in original order,\n///    with a sliver of context after each.\n/// 3) Hard caps and dedup to stay within MAX_KEEP_LINES.\npub fn extract_relevant_snippet(content: &str, lang: &str) -> String {\n    // Window the content for speed.\nfn score_line(l: &str, lang: &str) -> u8 {\n    let ll = l.to_ascii_lowercase();\nfn score_rust(l: &str, ll: &str) -> u8 {\n    if l.starts_with(\"///\") || l.starts_with(\"//!\") { return 9; }                     // docs\n    if ll.contains(\"todo\") || ll.contains(\"fixme\") { return 2; }\n    0\nfn score_python(l: &str, ll: &str) -> u8 {\n    if l.starts_with(\"\\\"\\\"\\\"\") || l.starts_with(\"'''\") || l.starts_with(\"#!\") || l.starts_with(\"# \") { return 9; } // docs/shebang\n    if ll.contains(\"todo\") || ll.contains(\"fixme\") { return 2; }\n    0\nfn score_js_ts(l: &str, _ll: &str) -> u8 {\n    if l.starts_with(\"/**\") || l.starts_with(\"* \") || l.starts_with(\"//\") { return 8; }       // docs/comments\nfn score_go(l: &str, _ll: &str) -> u8 {\n    if l.starts_with(\"//\") { return 7; }\nfn score_config(l: &str, _ll: &str) -> u8 {\n    if l.starts_with('[') || l.contains(\": \") || l.contains(\" = \") { return 5; }\nfn score_md(l: &str, _ll: &str) -> u8 {\n    if l.starts_with(\"# \") || l.starts_with(\"## \") { return 8; }\nfn score_generic(l: &str, _ll: &str) -> u8 {\n    if l.starts_with(\"//\") || l.starts_with(\"#\") || l.starts_with(\"--\") { return 6; }         // comments\nfn leading_doc_block(s: &str, lang: &str) -> Option<Vec<String>> {\n    match lang.to_ascii_lowercase().as_str() {\nfn leading_rust_docs(s: &str) -> Option<Vec<String>> {\n    let mut out = Vec::new();\nfn leading_python_docs(s: &str) -> Option<Vec<String>> {\n    let mut out = Vec::new();\nfn leading_js_docs(s: &str) -> Option<Vec<String>> {\n    let mut out = Vec::new();\nfn leading_md_head(s: &str) -> Option<Vec<String>> {\n    let mut out = Vec::new();\nfn leading_generic_head(s: &str) -> Option<Vec<String>> {\n    let mut out = Vec::new();\nfn normalize_doc_opt(v: Vec<String>) -> Option<Vec<String>> {\n    if v.is_empty() { return None; }\nfn normalize_doc(lines: Vec<String>) -> Vec<String> {\n    let mut v = Vec::new();\nfn push_lines(out: &mut Vec<String>, lines: Vec<String>) {\n    for l in lines {\nfn join(lines: &[String]) -> String {\n    lines.join(\"\\n\")\n#[cfg(test)]\nmod tests {\n    use super::*;\n    #[test]\n    fn rust_doc_capture() {\n        let s = \"//! Top module docs\\n/// more\\nfn main(){}\\n\";","tags":["rust","dir:src","ext:rs","paste"],"summary":"PASTE emitter: model-optimized prompt pack.","token_estimate":532,"role":"test","module":"snippet","imports":["super::*"],"exports":["extract_relevant_snippet"],"lines_total":386,"lines_nonblank":351,"rel_dir":"src","noise":false}
{"path":"src/types_view.rs","lang":"rust","sha1":"5bf678bd1ea41c9e8b8ec04c7ec37f90b96c4d2e","size":9284,"last_modified":"1757460793","snippet":"types_view.rs — renders \"Project Types\" grouped by source file, showing\nstructs/enums with field/variant names verbatim. Includes attributes on fields.\nAccepts JSON array or JSONL index files. Only `.rs` or `lang==\"rust\"` entries are parsed.\nAdd to Cargo.toml:\n```toml\n[dependencies]\nanyhow = \"1\"\nserde = { version = \"1\", features = [\"derive\"] }\nserde_json = \"1\"\nsyn = { version = \"2\", features = [\"full\", \"extra-traits\", \"printing\"] }\nquote = \"1\"\n```\n//! types_view.rs — renders \"Project Types\" grouped by source file, showing\n//! structs/enums with field/variant names verbatim. Includes attributes on fields.\n//!\n//! Accepts JSON array or JSONL index files. Only `.rs` or `lang==\"rust\"` entries are parsed.\n//!\n//! Add to Cargo.toml:\n//! ```toml\n//! [dependencies]\n//! anyhow = \"1\"\n//! serde = { version = \"1\", features = [\"derive\"] }\n//! serde_json = \"1\"\n//! syn = { version = \"2\", features = [\"full\", \"extra-traits\", \"printing\"] }\n//! quote = \"1\"\n//! ```\nuse std::collections::BTreeMap;\nuse std::fs;\nuse std::io::{self, Write};\nuse std::path::{Path, PathBuf};\nuse serde::Deserialize;\nuse syn::{visit::Visit, Attribute, Fields, Item, ItemEnum, ItemStruct};\nuse crate::map_view::indent;\n#[derive(Debug, Deserialize)]\nstruct FileIntentEntryMini {\n    path: String,\n    #[allow(dead_code)]\n    lang: Option<String>,\npub fn build_types_from_index(index_path: &Path, output_path: &Path) -> io::Result<()> {\n    let text = fs::read_to_string(index_path)?;\nfn resolve_path(root: &Path, p: &str) -> PathBuf { let pb = PathBuf::from(p); if pb.is_absolute() { pb } else { root.join(pb) } }\nfn to_rel(root: &Path, p: &Path) -> PathBuf { pathdiff::diff_paths(p, root).unwrap_or_else(|| p.to_path_buf()) }\n#[derive(Default)]\npub struct TypeCollector { pub out: Vec<Decl>, }\n    fn visit_item(&mut self, i: &'ast Item) {\n        match i {\nimpl TypeCollector {\n    fn push_struct(&mut self, s: &ItemStruct) {\n        let mut fields_out = Vec::new();\n    fn push_enum(&mut self, e: &ItemEnum) {\n        let public = matches!(e.vis, syn::Visibility::Public(_));","tags":["rust","dir:src","ext:rs"],"summary":"Filesystem / IO utilities.","token_estimate":368,"role":"lib","module":"types_view","imports":["std::collections::BTreeMap","std::fs","std::io::{self, Write}","std::path::{Path, PathBuf}","serde::Deserialize","syn::{visit::Visit, Attribute, Fields, Item, ItemEnum, ItemStruct}","crate::map_view::indent"],"exports":["build_types_from_index","TypeCollector"],"lines_total":268,"lines_nonblank":242,"rel_dir":"src","noise":false}
{"path":"src/util.rs","lang":"rust","sha1":"c8332ac8d5f779544bbcc1cdfc4c83375d61e16a","size":12625,"last_modified":"1757173109","snippet":"Utility layer: workdir slugs, filenames, timestamps, tagging, and misc helpers.\nNo side effects beyond explicit file writes. No global state.\n//! Utility layer: workdir slugs, filenames, timestamps, tagging, and misc helpers.\n//! No side effects beyond explicit file writes. No global state.\nuse std::{\n    fs::{self, File, Metadata},\n/// Best-effort current directory **slug**, safe for filenames.\n/// Falls back to env vars or \"project\". Lowercase, `[a-z0-9_-]`, collapsed `_`.\npub fn workdir_slug() -> String {\n    let cwd = std::env::current_dir().unwrap_or_else(|_| PathBuf::from(\".\"));\n/// Prefix an output filename with the workdir slug.\n/// Example: `prefixed_filename(\"PROJECT_TREE\", \"md\")` -> `indexer_PROJECT_TREE.md`\npub fn prefixed_filename(stem: &str, ext: &str) -> String {\n    format!(\n/// Join `base` + `rel`, normalizing `..` and stripping any leading separators in `rel`.\npub fn safe_join(base: &Path, rel: &Path) -> PathBuf {\n    let rel = rel.components().filter(|c| !matches!(c, Component::RootDir)).collect::<PathBuf>();\n/// RFC3339 (sortable) + a compact stamp string.\n/// Example: `20250810_140359 (2025-08-10T14:03:59-05:00)`\npub fn now_timestamp() -> String {\n    use chrono::{Local, SecondsFormat};\n    let now = Local::now();\n/// Compact, filesystem-safe UTC-agnostic local timestamp: `YYYYMMDD_HHMMSS`.\npub fn now_ts_compact() -> String {\n    use chrono::{Datelike, Local, Timelike};\n    let dt = Local::now();\n/// Modified time → UNIX seconds (as string). Falls back to created() or \"0\".\npub fn to_unix_epoch(meta: &Metadata) -> String {\n    fn secs(t: SystemTime) -> Option<String> {\n        t.duration_since(UNIX_EPOCH).ok().map(|d| d.as_secs().to_string())\n/// Atomic-ish write: write to `path.tmp`, fsync, then rename over `path`.\n/// Avoids torn writes on crash. Creates parent dirs as needed.\npub fn safe_write(path: &Path, contents: impl AsRef<[u8]>) -> io::Result<()> {\n    if let Some(parent) = path.parent() {\n/// Human-friendly bytes (SI), e.g., 1.2 KB, 3.4 MB. Exact for small numbers.\npub fn humanize_bytes(n: u64) -> String {\n    const UNITS: [&str; 6] = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"];\n/// Count non-empty source lines (for quick LOC estimates).\npub fn count_loc(text: &str) -> usize {\n    text.lines().filter(|l| !l.trim().is_empty()).count()\n/// Very cheap binary detector: if >1% NULs or many non-utf8 bytes, call it binary.\n/// Bound input slice to a window for speed.\npub fn is_probably_binary(bytes: &[u8]) -> bool {\n    let window = bytes.get(..8192).unwrap_or(bytes);","tags":["rust","dir:src","ext:rs"],"summary":"Utility helpers for the crate.","token_estimate":439,"role":"lib","module":"util","imports":["std::{","chrono::{Local, SecondsFormat}","chrono::{Datelike, Local, Timelike}"],"exports":["workdir_slug","prefixed_filename","safe_join","now_timestamp","now_ts_compact","to_unix_epoch","safe_write","humanize_bytes","count_loc","is_probably_binary"],"lines_total":361,"lines_nonblank":320,"rel_dir":"src","noise":false}
